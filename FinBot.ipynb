{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinaMwangi/finance_chatbot/blob/main/FinBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf65eqiuugPg"
      },
      "source": [
        "# Finance Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "H-sM3dynbGI6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "thEqMRZF6Fn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-keras"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u1Gr7mfvpe8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpJ5Q31eumIB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, create_optimizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The initializer RandomNormal is unseeded*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3Lnf0BWEGNH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDtG2vEzvOdl"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx1Zpkb4yUG1"
      },
      "outputs": [],
      "source": [
        "# Load dataset and display a sample\n",
        "dataset = load_dataset(\"virattt/financial-qa-10K\")[\"train\"]\n",
        "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_data = split_dataset[\"train\"]\n",
        "val_data = split_dataset[\"test\"]\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AUFt_g_z1rU"
      },
      "source": [
        "# Selecting my pretrained model\n",
        "The code loads a pretrained instruction-tuned language model (flan-t5-small) from Hugging Face:\n",
        "\n",
        "Defines the model name (flan-t5-small)\n",
        "\n",
        "Loading the tokenizer to convert text into tokens the model understands\n",
        "\n",
        "Loading the model itself (in TensorFlow) for sequence-to-sequence tasks like question answering, summarization, or translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAuHRhquy3tK"
      },
      "outputs": [],
      "source": [
        "# Selecting my model and tokenizer\n",
        "model_checkpoint = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B3Bde572wJr"
      },
      "source": [
        "# Preprocessing the dataset\n",
        "- The preprocess() function is preparing each dataset example for training the sequence-to-sequence model\n",
        "- Tokenizing both the input and the target (answer)\n",
        "\n",
        "- Setting the target tokens as labels, which the model uses to learn during training\n",
        "\n",
        "- Returning a tokenized dictionary ready for use in model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFQcfR0C3VAS"
      },
      "outputs": [],
      "source": [
        "def preprocess(example):\n",
        "    inputs = [\n",
        "        f\"Q: {q} Context: {c} A:\"\n",
        "        for q, c in zip(example[\"question\"], example[\"context\"])\n",
        "    ]\n",
        "    targets = example[\"answer\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_data.map(preprocess, batched=True)\n",
        "val_dataset = val_data.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgC5XLaV5AZR"
      },
      "source": [
        "# Batching\n",
        "Converting the tokenized dataset into a batched and ready to train format for TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6XYoc3a5Puz"
      },
      "outputs": [],
      "source": [
        "# Initialising the data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer, model=model, return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "# Converting the tokenized dataset into tf.data.Dataset objects\n",
        "tf_train_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=['input_ids', 'attention_mask', 'labels'],\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "tf_val_dataset = val_dataset.to_tf_dataset(\n",
        "    columns=[\"input_ids\", \"attention_mask\", 'labels'],\n",
        "    shuffle=False,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ2zT5ni6cTM"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7okqcfM6gqF"
      },
      "outputs": [],
      "source": [
        "# Creating an optimiser and a learning rate schedule\n",
        "# Training configuration\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "learning_rate = 3e-5\n",
        "train_data_size = len(train_dataset)\n",
        "\n",
        "# Total steps (use train_dataset if not shuffled externally)\n",
        "total_train_steps = (train_data_size // batch_size) * epochs\n",
        "warmup_steps = total_train_steps // 10\n",
        "\n",
        "# Create optimizer and scheduler\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=learning_rate,\n",
        "    num_train_steps=total_train_steps,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    weight_decay_rate=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer\n",
        ")"
      ],
      "metadata": {
        "id": "o92CY8n9T1po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPI5C36w7gds"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "history = model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_val_dataset,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWZVUbE0ewe"
      },
      "source": [
        "# Saving the Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsQG3iPy0dpL"
      },
      "outputs": [],
      "source": [
        "#Saving the pretrained model in Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Finance Chatbot\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Finance Chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Tokenizer and Fine_tuned model\n",
        "model_name = \"/content/drive/MyDrive/Finance Chatbot\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "bHB40er5t4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "foiPLh7Pc11K",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2tNO2gY4PIy"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Js8SBlz4R6Y"
      },
      "outputs": [],
      "source": [
        "# Loading evaluation metrics\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "em_metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "# Lists for metrics\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Generating responses\n",
        "for item in tqdm(val_data):\n",
        "    try:\n",
        "        question = item[\"question\"]\n",
        "        context = item[\"context\"]\n",
        "        reference_answer = item[\"answer\"]\n",
        "\n",
        "        prompt = f\"Q: {question} Context: {context} A:\"\n",
        "\n",
        "        # Tokenizing input\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"tf\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # Generating output\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64)\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # Saving for metrics\n",
        "        predictions.append(answer)\n",
        "        references.append(reference_answer.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing item: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Normalize both predictions and references\n",
        "norm_preds = [normalize_text(p) for p in predictions]\n",
        "norm_refs = [normalize_text(r) for r in references]\n",
        "\n",
        "# Evaluating ROUGE on normalized text\n",
        "rouge_result = rouge_metric.compute(\n",
        "    predictions=norm_preds,\n",
        "    references=norm_refs,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Evaluating BLEU on normalized text\n",
        "bleu_result = bleu_metric.compute(\n",
        "    predictions=norm_preds,\n",
        "    references=[[ref] for ref in norm_refs]\n",
        ")\n",
        "\n",
        "# Evaluating Exact Match\n",
        "em_result = em_metric.compute(predictions=norm_preds, references=norm_refs)\n",
        "\n",
        "# Print scores\n",
        "print(\"ROUGE-1 Score:\", round(rouge_result['rouge1'] * 100, 2))\n",
        "print(\"BLEU Score:\", round(bleu_result['bleu'] * 100, 2))\n",
        "print(\"Exact Match Score:\", round(em_result['exact_match'] * 100, 2))"
      ],
      "metadata": {
        "id": "kjKYQ2zdBUxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inferencing"
      ],
      "metadata": {
        "id": "t9FXuJt84rkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/Finance Chatbot\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "# Function to retrieve matching context\n",
        "def get_context_for_question(question):\n",
        "    for item in dataset:\n",
        "        if item[\"question\"].strip().lower() == question.strip().lower():\n",
        "            return item[\"context\"]\n",
        "    return \"No relevant context found.\"\n",
        "\n",
        "# Define the prediction function (inference)\n",
        "def generate_answer(question, chat_history):\n",
        "    context = get_context_for_question(question)\n",
        "    prompt = f\"Q: {question} Context: {context} A:\"\n",
        "\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"tf\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    chat_history.append((question, answer))\n",
        "    return \"\", chat_history\n"
      ],
      "metadata": {
        "id": "PmRnNP-R6s0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launching the chatbot interface\n",
        "with gr.Blocks(theme=gr.themes.Base()) as interface:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 💬 Finance QA Chatbot\n",
        "        Ask a finance-related question and get an accurate, concise response.\n",
        "        Built using a fine-tuned T5 Transformer on financial Q&A data.\n",
        "        \"\"\",\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Finance Chatbot\", height=400, bubble_full_width=False)\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=8):\n",
        "            question_box = gr.Textbox(\n",
        "                placeholder=\"Ask a finance question...\", show_label=False, lines=2\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            submit_btn = gr.Button(\"Send\")\n",
        "\n",
        "    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Chat state\n",
        "    state = gr.State([])\n",
        "\n",
        "    # Bind function\n",
        "    submit_btn.click(\n",
        "        generate_answer,\n",
        "        inputs=[question_box, state],\n",
        "        outputs=[question_box, chatbot],\n",
        "    )\n",
        "\n",
        "    clear_btn.click(lambda: [], inputs=[], outputs=[chatbot, state])\n",
        "\n",
        "# Run app\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "cDXarHTXOQZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "mount_file_id": "1sK9Pk8VB4WoQlAupUfV2Sy5ptFZElx-9",
      "authorship_tag": "ABX9TyM3pHC3qwcx4MzF33E/uzQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}